{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BedrockRuntimeWrapper:\n",
    "    def __init__(self, bedrock_runtime_client):\n",
    "        \"\"\"\n",
    "        :param bedrock_runtime_client: A low-level client representing Amazon Bedrock Runtime.\n",
    "                                       Describes the API operations for running inference using\n",
    "                                       Bedrock models.\n",
    "        \"\"\"\n",
    "        self.bedrock_runtime_client = bedrock_runtime_client\n",
    "        \n",
    "    def invoke_mistral_7b(self, prompt):\n",
    "        \"\"\"\n",
    "        Invokes the Mistral 7B model to run an inference using the input\n",
    "        provided in the request body.\n",
    "\n",
    "        :param prompt: The prompt that you want Mistral to complete.\n",
    "        :return: List of inference responses from the model.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            # Mistral instruct models provide optimal results when\n",
    "            # embedding the prompt into the following template:\n",
    "            instruction = f\"<s>[INST] {prompt} [/INST]\"\n",
    "\n",
    "            model_id = \"mistral.mistral-7b-instruct-v0:2\"\n",
    "\n",
    "            body = {\n",
    "                \"prompt\": instruction,\n",
    "                \"max_tokens\": 200,\n",
    "                \"temperature\": 0.5,\n",
    "            }\n",
    "\n",
    "            response = self.bedrock_runtime_client.invoke_model(\n",
    "                modelId=model_id, body=json.dumps(body)\n",
    "            )\n",
    "\n",
    "            response_body = json.loads(response[\"body\"].read())\n",
    "            outputs = response_body.get(\"outputs\")\n",
    "\n",
    "            completions = [output[\"text\"] for output in outputs]\n",
    "\n",
    "            return completions\n",
    "\n",
    "        except ClientError:\n",
    "            logger.error(\"Couldn't invoke Mistral 7B\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke(wrapper, model_id, prompt):\n",
    "    print(\"-\" * 88)\n",
    "    print(f\"Invoking: {model_id}\")\n",
    "    print(\"Prompt: \" + prompt)\n",
    "\n",
    "    try:\n",
    "        if model_id == \"anthropic.claude-v2\":\n",
    "            completion = wrapper.invoke_claude(prompt)\n",
    "            print(\"Completion: \" + completion)\n",
    "\n",
    "        elif model_id == \"ai21.j2-mid-v1\":\n",
    "            completion = wrapper.invoke_jurassic2(prompt)\n",
    "            print(\"Completion: \" + completion)\n",
    "\n",
    "        elif model_id == \"meta.llama2-13b-chat-v1\":\n",
    "            completion = wrapper.invoke_llama2(prompt)\n",
    "            print(\"Completion: \" + completion)\n",
    "\n",
    "        elif model_id == \"mistral.mistral-7b-instruct-v0:2\":\n",
    "            completions = wrapper.invoke_mistral_7b(prompt)\n",
    "            for completion in completions:\n",
    "                print(\"Completion: \" + completion)\n",
    "\n",
    "        elif model_id == \"mistral.mixtral-8x7b-instruct-v0:1\":\n",
    "            completions = wrapper.invoke_mixtral_8x7b(prompt)\n",
    "            for completion in completions:\n",
    "                print(\"Completion: \" + completion)\n",
    "        \n",
    "        return completions\n",
    "\n",
    "    except ClientError:\n",
    "        logger.exception(\"Couldn't invoke model %s\", model_id)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------\n",
      "Invoking: mistral.mistral-7b-instruct-v0:2\n",
      "Prompt: Verify the following statement: Donald Trump is 60 years old\n",
      "Completion:  I cannot directly verify the statement as I do not have access to real-time or personal information about individuals. However, I can tell you that Donald Trump was born on June 14, 1946. To calculate his age, you can subtract his birth year from the current year. As of now, the current year is 2022, so the calculation would be: 2022 - 1946 = 76. Therefore, Donald Trump would be 76 years old. The statement that he is 60 years old is incorrect.\n"
     ]
    }
   ],
   "source": [
    "client = boto3.client(service_name=\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "wrapper = BedrockRuntimeWrapper(client)\n",
    "\n",
    "text_generation_prompt  = \"Verify the following statement: Donald Trump is 60 years old\"\n",
    "model_id = \"mistral.mistral-7b-instruct-v0:2\"\n",
    "\n",
    "invoke(wrapper, model_id, text_generation_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
